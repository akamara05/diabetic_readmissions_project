{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0a40df1",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc56c554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78f41230",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"AIzaSyCBy0QpyTKfxPAIGJqPwMQ0_QATkhdilgk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdb32c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#channel_id = \"UCbXgNpp0jedKWcQiULLbDTA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b17e063",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_id = \"UCK2ACorzpH-igxuHZ2ObCEA\" #Its Judy's life "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525c9edf",
   "metadata": {},
   "source": [
    "__Gathering Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b833b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function to pull YT statistics for a given YT channel \n",
    "class YTstats:\n",
    "    def __init__(self,api_key,channel_id): #passed api key and channel id. \n",
    "        self.api_key = api_key #storing api key\n",
    "        self.channel_id = channel_id #storing channel id \n",
    "        self.channel_statistics = None #Creating an object that will eventually store channel statistics\n",
    "        self.video_data = None #Creating an object that will eventually store video statistics\n",
    "        \n",
    "#pulling general channel statistics using the api key and chanel ID. Name of channel, No. of subscribers,number of views and number of videos. \n",
    "    def get_channel_statistics(self): #creating function to gather channel stats. \n",
    "        url = f'https://www.googleapis.com/youtube/v3/channels?part=statistics&id={self.channel_id}&key={self.api_key}' #creating a url object to get channel stats.\n",
    "#         print(url)\n",
    "        json_url = requests.get(url) #creating a get request for the YT channel \n",
    "        print(json_url.status_code)\n",
    "#         print(json_url.text)\n",
    "        data =json.loads(json_url.text) #loads the actual text from the url\n",
    "#         print(data)\n",
    "        try:\n",
    "            data = data[\"items\"][0][\"statistics\"] #Used the items key(from url we requested), to gather the first item in that list. Also used the 'statistics' key, and gathered all info under that key. This is placed in a 'try and except' block, to avoid crashes if any errors.  \n",
    "        except:\n",
    "            data = None  #if any errors no information is returned. \n",
    "        self.channel_statistics = data #information gathered is stored in the channel stats object. \n",
    "        return data\n",
    "    \n",
    "#Gets list of videos ids per channel, limited to 50 videos per page. \n",
    "    def get_channel_video_data(self):\n",
    "        channel_videos = self._get_channel_video(limit = 50) #calls the '_get_channel_video' helper function below.\n",
    "        print(channel_videos)\n",
    "        parts = [\"snippet\",\"statistics\",\"contentDetails\"] #Parts from the video we want. The snippet provides the publish date, video title, and description. Video stats. Content Deatails such as the duration. \n",
    "        for video_id in channel_videos:\n",
    "            for part in parts:\n",
    "                data = self._get_single_video_data(video_id, part) #calling the '_get_single_video_data' helper functionn\n",
    "                channel_videos[video_id].update(data) #update data gathered from '_get_single_video_data' helper function below. \n",
    "            \n",
    "            self.video_data = channel_videos #store video data in this object. \n",
    "            return channel_videos \n",
    "        \n",
    "            \n",
    "#helper function\n",
    "    def _get_single_video_data(self,video_id,part):\n",
    "        url = f\"https://www.googleapis.com/youtube/v3/videos?part={part}&id={video_id}&key={self.api_key}\"\n",
    "        json_url = requests.get(url)  #create get url using the video id and api key \n",
    "        data = json.loads(json_url.text) #loads the data\n",
    "        try: \n",
    "            data = data['items'][0][part] #items key, pulls first value in this list. Part is \"snippet\",\"statistics\",\"contentDetails\" from function right above. \n",
    "        except:\n",
    "            print('Oh no!')\n",
    "            data = dict()\n",
    "            \n",
    "        return data    \n",
    "    \n",
    "#This function is a 'helper method' function.  \n",
    "    def _get_channel_video(self, limit= None):\n",
    "        url = f'https://www.googleapis.com/youtube/v3/search?key={self.api_key}&channelId={self.channel_id}&part=id&order=date'#pass api key and channel id. Also include the video id and sorted according to date. \n",
    "        if limit is not None and isinstance(limit,int): #If no limit is specified will only give you info for the first 5 videos. Check to see if limit is not none. \n",
    "            url += \"&maxResults=\" + str(limit) #inclduing the max results parameter allows me to get past the 5 video results default. \n",
    "        \n",
    "        vid,npt = self._get_channel_video_per_page(url) #video and next page token(npt)\n",
    "        indx = 0\n",
    "        while(npt is not None and indx <10): #limited to avoid any errors whilst using a while loop. \n",
    "            nexturl = url + \"&PageToken=\" + npt #appends the next page token(npt) to the url \n",
    "            next_vid,npt = self._get_channel_video_per_page(nexturl) #returns the videos from the next page if any. \n",
    "            vid.update(next_vid)#append the videos from the next page. \n",
    "            indx +=1 #increase the counter. \n",
    "        return vid\n",
    "       \n",
    " #This is another helper function. It pulls a list of video ids on each page if more than one page/more than 50 videos.    \n",
    "    def _get_channel_video_per_page(self,url):\n",
    "        json_url = requests.get(url) #creating get request to return video ids for each page. \n",
    "        data = json.loads(json_url.text)\n",
    "        channel_videos = dict() #empty dictionary to gather list of video ids. \n",
    "        if 'items' not in data:\n",
    "            return channel_videos, None\n",
    "        \n",
    "        item_data = data['items'] #using the items key to gather video ids listed under it.  \n",
    "        nextPageToken = data.get(\"nextPageToken\",None) #gets video ids for pages past page 1\n",
    "        for item in item_data:\n",
    "            try:\n",
    "                kind = item['id']['kind'] #channel id and kind is youtube#video\n",
    "                if kind == 'youtube#video': #want videos and not playlists\n",
    "                    video_id = item['id']['videoId'] #extraction of unique channel id and the corresponding video ids. \n",
    "                    channel_videos[video_id] = dict()\n",
    "            except KeyError:\n",
    "                print(\"Oh no!\")\n",
    "                \n",
    "        return channel_videos, nextPageToken \n",
    "    \n",
    "  #compiles all the information gathered and dumps it into a JSON file   \n",
    "    def dump(self):\n",
    "        if self.channel_statistics is None or self.video_data is None: #check if 'self.channel_statistics'(above) or video data is none print,'data is none'\n",
    "            print('data is none')\n",
    "            return  #if not return the following below. \n",
    "        \n",
    "        combined_data = {self.channel_id:{\"channel_statistics\":self.channel_statistics,\"video_data\":self.video_data}} #combines channel stats and video stats for all videos on the channel.\n",
    "        \n",
    "        channel_title = self.video_data.popitem()[1].get('channelTitle', self.channel_id) #Want to get the channel title from the video data. Created a popitem that returns a tuple, having a key and a value. If it can't get 'channelTitle' as a fall back it will use self.channel_id\n",
    "        channel_title = channel_title.replace(\" \",\"_\").lower() #converted to lower cases.\n",
    "        file_name = channel_title + '.json'\n",
    "        with open(file_name, 'w') as f:\n",
    "            json.dump(combined_data, f, indent = 4)\n",
    "        \n",
    "        print('file dumped')     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e524f",
   "metadata": {},
   "source": [
    "Sources:\n",
    "* [Part 1](https://www.youtube.com/watch?v=5qtC-tsQ-wE&list=RDCMUCbXgNpp0jedKWcQiULLbDTA&index=4)\n",
    "* [Part 2](https://www.youtube.com/watch?v=4FwXqOT4-FM&t=9s)\n",
    "* [Part 3](https://www.youtube.com/watch?v=itiRd7iZ2_M&list=RDCMUCbXgNpp0jedKWcQiULLbDTA&index=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fc56fa",
   "metadata": {},
   "source": [
    "### Testing of functions to gather data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39730c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'mW6FerTLT1g': {}, 'kY3NRqZE150': {}, 'MoPi8Quedos': {}, 'HXi6CVWsv3U': {}, 'Wsp142ixEdE': {}, 'BJMtM1CGpRg': {}, '_eEGBWjSuVE': {}, 'WIr9xbjS-Ro': {}, 'M2BVo2Fydcg': {}, 'VjhQsg80V-g': {}, 'cUZlio5789Y': {}, 'dKgrO8-5ElI': {}, 'bfX0JvSj7qo': {}, 'ChzAJ8jsJbY': {}, '6qcZPnLD73k': {}, '6DOtCstb0l8': {}, 'clmiNsET2vw': {}, 'ZJ3yYp0-rFI': {}, 'DDrQgyPgr7A': {}, '25sE5nDOfMY': {}, 'NpBeNnwh_HY': {}, 'iD8aXKtKy2Y': {}, 'pLE83VXhodU': {}, 'ug4O5fmKCy8': {}, 'knbhKExFU3Y': {}, 'BurKAuF4TvA': {}, 'L_3s-W5iqZk': {}, 'TJ_w921hPho': {}, 'eHfe4ALekGA': {}, 'KZfC_EGNUbE': {}, 'SNgEw9nylwE': {}, 'CJLi8tm7vWE': {}, 'VNTazFzlSIQ': {}, '1kPj04-7Y2U': {}, 'Hbx_I4LX554': {}, 'wWJ9_ZgcziA': {}, 'cQtkHPr-Byk': {}, 'PAPkU3CRIXo': {}, '7HGOkmvb5mw': {}, 'PBRwQ-QFcjc': {}, 'yqziZAGqh-c': {}, 'blZFIUAbrwg': {}, 'xTOh9mHKXm8': {}, 'E2hEjUrspz8': {}, 'v6ikCVyFdRM': {}, '5_mxUnekUyA': {}, '-ELBZwUwIXM': {}, 'o9Kv6A3fTqA': {}, 'm8HLC2DQwbI': {}, 'q03Aky3VoHc': {}, 'vL0VPwIzBSU': {}, 'biYu1PniVNw': {}, 'D-3OHXPaZ1k': {}, 'amzI8Zm3Hig': {}, 'NyvSiObUwmA': {}}\n",
      "file dumped\n"
     ]
    }
   ],
   "source": [
    "yt = YTstats(api_key, channel_id)\n",
    "yt.get_channel_statistics()\n",
    "yt.get_channel_video_data()\n",
    "yt.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa218bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
